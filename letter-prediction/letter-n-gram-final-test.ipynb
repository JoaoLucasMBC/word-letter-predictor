{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final N-gram model Prediction**\n",
    "\n",
    "After testing the majority voting algorithm, we discussed the possibility of hitting a plateau for such a simple model.\n",
    "\n",
    "Also, we concluded that, in a language, it is mandatory that the set of words is finite. Should we worry about overfitting to split the dataset? From the moment we have a big population of combination of prefixes, the samples might not be necessary. If we remove the most frequent words, we are not avoiding overfitting, but we are removing important data.\n",
    "\n",
    "Therefore, for a last test, we are going to approach creating a final dataset with a big conversational vocab, and smooth it with a sort of \"Laplace smoothing\", by also scanning the most frequent words of the language once. Then, we will test by training with a split, and then without to see the effects of \"overfitting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our conversational dataset, I'll use the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus?select=movie_lines.tsv) from Kaggle. Please download the `movie_lines.tsv` file and place it in the `data/raw` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neodr\\AppData\\Local\\Temp\\ipykernel_32604\\3843951092.py:2: ParserWarning: Skipping line 32288: expected 5 fields, saw 7\n",
      "Skipping line 32351: expected 5 fields, saw 6\n",
      "Skipping line 32390: expected 5 fields, saw 6\n",
      "Skipping line 32583: expected 5 fields, saw 6\n",
      "Skipping line 32585: expected 5 fields, saw 6\n",
      "Skipping line 35684: expected 5 fields, saw 6\n",
      "Skipping line 62132: expected 5 fields, saw 6\n",
      "Skipping line 86637: expected 5 fields, saw 6\n",
      "Skipping line 86722: expected 5 fields, saw 6\n",
      "Skipping line 86914: expected 5 fields, saw 6\n",
      "Skipping line 86960: expected 5 fields, saw 6\n",
      "Skipping line 87010: expected 5 fields, saw 6\n",
      "Skipping line 87011: expected 5 fields, saw 6\n",
      "Skipping line 87086: expected 5 fields, saw 6\n",
      "Skipping line 120607: expected 5 fields, saw 6\n",
      "Skipping line 120719: expected 5 fields, saw 7\n",
      "Skipping line 120739: expected 5 fields, saw 6\n",
      "Skipping line 120783: expected 5 fields, saw 6\n",
      "Skipping line 130284: expected 5 fields, saw 7\n",
      "Skipping line 131048: expected 5 fields, saw 6\n",
      "\n",
      "  df_movies = pd.read_csv('../data/raw/movie_lines.tsv', sep='\\t', on_bad_lines='warn', names=['lineID', 'characterID', 'movieID', 'character', 'text'])\n",
      "C:\\Users\\neodr\\AppData\\Local\\Temp\\ipykernel_32604\\3843951092.py:2: ParserWarning: Skipping line 150955: expected 5 fields, saw 8\n",
      "Skipping line 162777: expected 5 fields, saw 11\n",
      "Skipping line 162778: expected 5 fields, saw 11\n",
      "Skipping line 162830: expected 5 fields, saw 9\n",
      "Skipping line 162864: expected 5 fields, saw 11\n",
      "Skipping line 162883: expected 5 fields, saw 8\n",
      "Skipping line 190170: expected 5 fields, saw 6\n",
      "Skipping line 195924: expected 5 fields, saw 6\n",
      "Skipping line 196054: expected 5 fields, saw 6\n",
      "Skipping line 196088: expected 5 fields, saw 6\n",
      "Skipping line 200335: expected 5 fields, saw 6\n",
      "Skipping line 200453: expected 5 fields, saw 6\n",
      "Skipping line 200863: expected 5 fields, saw 6\n",
      "Skipping line 200913: expected 5 fields, saw 6\n",
      "Skipping line 200934: expected 5 fields, saw 6\n",
      "Skipping line 200991: expected 5 fields, saw 6\n",
      "Skipping line 209214: expected 5 fields, saw 6\n",
      "Skipping line 209271: expected 5 fields, saw 6\n",
      "Skipping line 212476: expected 5 fields, saw 17\n",
      "Skipping line 212477: expected 5 fields, saw 12\n",
      "Skipping line 212490: expected 5 fields, saw 12\n",
      "Skipping line 212493: expected 5 fields, saw 10\n",
      "Skipping line 212514: expected 5 fields, saw 9\n",
      "Skipping line 212515: expected 5 fields, saw 12\n",
      "Skipping line 212556: expected 5 fields, saw 12\n",
      "Skipping line 212562: expected 5 fields, saw 8\n",
      "Skipping line 212567: expected 5 fields, saw 12\n",
      "Skipping line 212575: expected 5 fields, saw 12\n",
      "Skipping line 212582: expected 5 fields, saw 14\n",
      "Skipping line 212595: expected 5 fields, saw 11\n",
      "Skipping line 212600: expected 5 fields, saw 8\n",
      "Skipping line 212652: expected 5 fields, saw 11\n",
      "Skipping line 212655: expected 5 fields, saw 8\n",
      "Skipping line 212696: expected 5 fields, saw 9\n",
      "Skipping line 212700: expected 5 fields, saw 35\n",
      "Skipping line 212704: expected 5 fields, saw 9\n",
      "Skipping line 212736: expected 5 fields, saw 26\n",
      "Skipping line 212737: expected 5 fields, saw 12\n",
      "Skipping line 212738: expected 5 fields, saw 34\n",
      "Skipping line 212740: expected 5 fields, saw 7\n",
      "Skipping line 212741: expected 5 fields, saw 11\n",
      "Skipping line 212742: expected 5 fields, saw 11\n",
      "Skipping line 212743: expected 5 fields, saw 12\n",
      "Skipping line 212744: expected 5 fields, saw 11\n",
      "Skipping line 212745: expected 5 fields, saw 10\n",
      "Skipping line 212746: expected 5 fields, saw 17\n",
      "Skipping line 212747: expected 5 fields, saw 10\n",
      "Skipping line 212748: expected 5 fields, saw 21\n",
      "Skipping line 212749: expected 5 fields, saw 14\n",
      "Skipping line 212755: expected 5 fields, saw 9\n",
      "Skipping line 212756: expected 5 fields, saw 10\n",
      "Skipping line 212757: expected 5 fields, saw 14\n",
      "Skipping line 212758: expected 5 fields, saw 10\n",
      "Skipping line 212759: expected 5 fields, saw 7\n",
      "Skipping line 212760: expected 5 fields, saw 17\n",
      "Skipping line 212768: expected 5 fields, saw 9\n",
      "Skipping line 212774: expected 5 fields, saw 12\n",
      "Skipping line 212775: expected 5 fields, saw 11\n",
      "Skipping line 212784: expected 5 fields, saw 11\n",
      "Skipping line 212790: expected 5 fields, saw 29\n",
      "Skipping line 212791: expected 5 fields, saw 12\n",
      "Skipping line 212888: expected 5 fields, saw 12\n",
      "Skipping line 212895: expected 5 fields, saw 7\n",
      "Skipping line 212901: expected 5 fields, saw 8\n",
      "Skipping line 212909: expected 5 fields, saw 21\n",
      "Skipping line 212910: expected 5 fields, saw 19\n",
      "Skipping line 212912: expected 5 fields, saw 19\n",
      "Skipping line 212913: expected 5 fields, saw 14\n",
      "Skipping line 212914: expected 5 fields, saw 19\n",
      "Skipping line 212915: expected 5 fields, saw 19\n",
      "Skipping line 212916: expected 5 fields, saw 12\n",
      "Skipping line 212918: expected 5 fields, saw 18\n",
      "Skipping line 220002: expected 5 fields, saw 6\n",
      "Skipping line 223476: expected 5 fields, saw 6\n",
      "Skipping line 223670: expected 5 fields, saw 6\n",
      "Skipping line 227241: expected 5 fields, saw 6\n",
      "Skipping line 227687: expected 5 fields, saw 6\n",
      "Skipping line 227795: expected 5 fields, saw 6\n",
      "Skipping line 227850: expected 5 fields, saw 6\n",
      "Skipping line 228956: expected 5 fields, saw 6\n",
      "Skipping line 229142: expected 5 fields, saw 6\n",
      "Skipping line 229252: expected 5 fields, saw 6\n",
      "Skipping line 229283: expected 5 fields, saw 6\n",
      "Skipping line 229343: expected 5 fields, saw 6\n",
      "Skipping line 237190: expected 5 fields, saw 6\n",
      "Skipping line 237212: expected 5 fields, saw 6\n",
      "Skipping line 237475: expected 5 fields, saw 6\n",
      "Skipping line 237680: expected 5 fields, saw 6\n",
      "Skipping line 239909: expected 5 fields, saw 6\n",
      "Skipping line 242513: expected 5 fields, saw 6\n",
      "Skipping line 242621: expected 5 fields, saw 6\n",
      "Skipping line 252471: expected 5 fields, saw 10\n",
      "Skipping line 254823: expected 5 fields, saw 6\n",
      "Skipping line 257362: expected 5 fields, saw 6\n",
      "Skipping line 260304: expected 5 fields, saw 11\n",
      "Skipping line 261241: expected 5 fields, saw 11\n",
      "Skipping line 261309: expected 5 fields, saw 6\n",
      "Skipping line 261343: expected 5 fields, saw 6\n",
      "Skipping line 261432: expected 5 fields, saw 6\n",
      "Skipping line 261461: expected 5 fields, saw 6\n",
      "Skipping line 261520: expected 5 fields, saw 14\n",
      "Skipping line 261521: expected 5 fields, saw 13\n",
      "Skipping line 261525: expected 5 fields, saw 13\n",
      "Skipping line 261533: expected 5 fields, saw 6\n",
      "Skipping line 261558: expected 5 fields, saw 7\n",
      "Skipping line 261560: expected 5 fields, saw 7\n",
      "Skipping line 262063: expected 5 fields, saw 6\n",
      "Skipping line 262098: expected 5 fields, saw 6\n",
      "\n",
      "  df_movies = pd.read_csv('../data/raw/movie_lines.tsv', sep='\\t', on_bad_lines='warn', names=['lineID', 'characterID', 'movieID', 'character', 'text'])\n",
      "C:\\Users\\neodr\\AppData\\Local\\Temp\\ipykernel_32604\\3843951092.py:2: ParserWarning: Skipping line 269343: expected 5 fields, saw 6\n",
      "Skipping line 269435: expected 5 fields, saw 6\n",
      "Skipping line 278994: expected 5 fields, saw 9\n",
      "Skipping line 278995: expected 5 fields, saw 9\n",
      "Skipping line 278996: expected 5 fields, saw 15\n",
      "Skipping line 279001: expected 5 fields, saw 15\n",
      "Skipping line 279038: expected 5 fields, saw 16\n",
      "Skipping line 279040: expected 5 fields, saw 10\n",
      "Skipping line 279041: expected 5 fields, saw 19\n",
      "Skipping line 279045: expected 5 fields, saw 13\n",
      "Skipping line 279047: expected 5 fields, saw 9\n",
      "Skipping line 279058: expected 5 fields, saw 10\n",
      "Skipping line 279069: expected 5 fields, saw 11\n",
      "Skipping line 279087: expected 5 fields, saw 11\n",
      "Skipping line 279094: expected 5 fields, saw 12\n",
      "Skipping line 279116: expected 5 fields, saw 22\n",
      "Skipping line 279131: expected 5 fields, saw 16\n",
      "Skipping line 279152: expected 5 fields, saw 10\n",
      "Skipping line 279155: expected 5 fields, saw 19\n",
      "Skipping line 279159: expected 5 fields, saw 9\n",
      "Skipping line 279199: expected 5 fields, saw 36\n",
      "Skipping line 279211: expected 5 fields, saw 9\n",
      "Skipping line 279213: expected 5 fields, saw 13\n",
      "Skipping line 279214: expected 5 fields, saw 9\n",
      "Skipping line 279233: expected 5 fields, saw 14\n",
      "Skipping line 279253: expected 5 fields, saw 15\n",
      "Skipping line 279345: expected 5 fields, saw 13\n",
      "Skipping line 279346: expected 5 fields, saw 10\n",
      "Skipping line 279379: expected 5 fields, saw 9\n",
      "Skipping line 279434: expected 5 fields, saw 17\n",
      "Skipping line 279440: expected 5 fields, saw 14\n",
      "Skipping line 279442: expected 5 fields, saw 14\n",
      "Skipping line 279443: expected 5 fields, saw 10\n",
      "Skipping line 279452: expected 5 fields, saw 16\n",
      "Skipping line 279455: expected 5 fields, saw 9\n",
      "Skipping line 279456: expected 5 fields, saw 12\n",
      "Skipping line 279457: expected 5 fields, saw 10\n",
      "Skipping line 279458: expected 5 fields, saw 10\n",
      "\n",
      "  df_movies = pd.read_csv('../data/raw/movie_lines.tsv', sep='\\t', on_bad_lines='warn', names=['lineID', 'characterID', 'movieID', 'character', 'text'])\n"
     ]
    }
   ],
   "source": [
    "# read the tsv file\n",
    "df_movies = pd.read_csv('../data/raw/movie_lines.tsv', sep='\\t', on_bad_lines='warn', names=['lineID', 'characterID', 'movieID', 'character', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lineID</th>\n",
       "      <th>characterID</th>\n",
       "      <th>movieID</th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>They do to!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>She okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>L590</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Queen Harry?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>L589\\tu4\\tm0\\tJOEY\\tSo yeah I've got the Sears...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>L397</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Hopefully.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>L396</td>\n",
       "      <td>u4</td>\n",
       "      <td>m0</td>\n",
       "      <td>JOEY</td>\n",
       "      <td>Exactly  So you going to Bogey Lowenbrau's thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>L395</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Expensive?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lineID characterID movieID  \\\n",
       "0                                               L1045          u0      m0   \n",
       "1                                               L1044          u2      m0   \n",
       "2                                                L985          u0      m0   \n",
       "3                                                L984          u2      m0   \n",
       "4                                                L925          u0      m0   \n",
       "..                                                ...         ...     ...   \n",
       "95                                               L590          u0      m0   \n",
       "96  L589\\tu4\\tm0\\tJOEY\\tSo yeah I've got the Sears...         NaN     NaN   \n",
       "97                                               L397          u0      m0   \n",
       "98                                               L396          u4      m0   \n",
       "99                                               L395          u0      m0   \n",
       "\n",
       "   character                                               text  \n",
       "0     BIANCA                                       They do not!  \n",
       "1    CAMERON                                        They do to!  \n",
       "2     BIANCA                                         I hope so.  \n",
       "3    CAMERON                                          She okay?  \n",
       "4     BIANCA                                          Let's go.  \n",
       "..       ...                                                ...  \n",
       "95    BIANCA                                       Queen Harry?  \n",
       "96       NaN                                                NaN  \n",
       "97    BIANCA                                         Hopefully.  \n",
       "98      JOEY  Exactly  So you going to Bogey Lowenbrau's thi...  \n",
       "99    BIANCA                                         Expensive?  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 304k lines, we lost almost 10k lines due to misconfigurations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293202, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "\n",
    "First, we need to preprocess the data. We will remove the lines with missing values, remove all punctuation, all words with numbers and just repeating letters (such as \"mmmmmmm\") and lowercase all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that are not needed\n",
    "df_movies_processed = df_movies.drop(columns=['lineID', 'characterID', 'movieID', 'character'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaN values\n",
    "df_movies_processed = df_movies_processed.dropna()\n",
    "\n",
    "# Removing empty strings\n",
    "df_movies_processed = df_movies_processed[df_movies_processed['text'] != ' ']\n",
    "\n",
    "# Removing special characters but keep ' and spaces\n",
    "df_movies_processed['text'] = df_movies_processed['text'].str.replace('[^a-zA-Z0-9\\' ]', '', regex=True)\n",
    "\n",
    "# Remove \"-\"\n",
    "df_movies_processed['text'] = df_movies_processed['text'].str.replace('-', '')\n",
    "\n",
    "# lower case\n",
    "df_movies_processed['text'] = df_movies_processed['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>they do not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>they do to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i hope so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>let's go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>it's more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>perm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>patrick  is that a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>it's just you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>is that woman a complete fruitloop or is it ju...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                                          they do not\n",
       "1                                           they do to\n",
       "2                                            i hope so\n",
       "3                                             she okay\n",
       "4                                             let's go\n",
       "..                                                 ...\n",
       "100                                          it's more\n",
       "101                                               perm\n",
       "102                                 patrick  is that a\n",
       "103                                      it's just you\n",
       "104  is that woman a complete fruitloop or is it ju...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies_processed.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a list with all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = df_movies_processed['text'].str.split(' ').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['they', 'do', 'not'],\n",
       " ['they', 'do', 'to'],\n",
       " ['i', 'hope', 'so'],\n",
       " ['she', 'okay'],\n",
       " [\"let's\", 'go'],\n",
       " ['wow'],\n",
       " ['okay', '', \"you're\", 'gonna', 'need', 'to', 'learn', 'how', 'to', 'lie'],\n",
       " ['no'],\n",
       " ['like', 'my', 'fear', 'of', 'wearing', 'pastels'],\n",
       " ['what', 'good', 'stuff']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [item for sublist in words_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'do', 'not', 'they', 'do', 'to', 'i', 'hope', 'so', 'she']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove words that:\n",
    "\n",
    "* Appear less than 5 times  \n",
    "* Have more than 15 characters  \n",
    "* Have only one type of letter (such as \"mmmmmmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = {}\n",
    "\n",
    "for word in words_list:\n",
    "    if word in word_frequency:\n",
    "        word_frequency[word] += 1\n",
    "    else:\n",
    "        word_frequency[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'they': 11277,\n",
       " 'do': 21291,\n",
       " 'not': 18161,\n",
       " 'to': 74812,\n",
       " 'i': 95142,\n",
       " 'hope': 942,\n",
       " 'so': 12330,\n",
       " 'she': 7992,\n",
       " 'okay': 4256,\n",
       " \"let's\": 2288,\n",
       " 'go': 9263,\n",
       " 'wow': 240,\n",
       " '': 117328,\n",
       " \"you're\": 12612,\n",
       " 'gonna': 4035,\n",
       " 'need': 3667,\n",
       " 'learn': 402,\n",
       " 'how': 9673,\n",
       " 'lie': 422,\n",
       " 'no': 17927,\n",
       " 'like': 13893,\n",
       " 'my': 19172,\n",
       " 'fear': 254,\n",
       " 'of': 36042,\n",
       " 'wearing': 240,\n",
       " 'pastels': 2,\n",
       " 'what': 29794,\n",
       " 'good': 6817,\n",
       " 'stuff': 1016,\n",
       " 'figured': 305,\n",
       " \"you'd\": 1379,\n",
       " 'get': 13203,\n",
       " 'the': 90717,\n",
       " 'eventually': 78,\n",
       " 'thank': 1826,\n",
       " 'god': 2164,\n",
       " 'if': 12205,\n",
       " 'had': 5121,\n",
       " 'hear': 1700,\n",
       " 'one': 9472,\n",
       " 'more': 4137,\n",
       " 'story': 834,\n",
       " 'about': 13023,\n",
       " 'your': 19457,\n",
       " 'coiffure': 3,\n",
       " 'me': 29558,\n",
       " 'this': 22600,\n",
       " 'endless': 15,\n",
       " 'blonde': 77,\n",
       " 'babble': 5,\n",
       " \"i'm\": 20784,\n",
       " 'boring': 107,\n",
       " 'myself': 1121,\n",
       " 'crap': 197,\n",
       " 'you': 119540,\n",
       " 'listen': 1711,\n",
       " 'always': 2366,\n",
       " 'been': 6067,\n",
       " 'selfish': 41,\n",
       " 'but': 15755,\n",
       " 'then': 5561,\n",
       " \"that's\": 10112,\n",
       " 'all': 14228,\n",
       " 'say': 5444,\n",
       " 'well': 9099,\n",
       " 'never': 5046,\n",
       " 'wanted': 1676,\n",
       " 'out': 12352,\n",
       " 'with': 15855,\n",
       " \"'me\": 2,\n",
       " 'did': 8081,\n",
       " 'was': 18075,\n",
       " 'tons': 32,\n",
       " 'have': 20088,\n",
       " 'fun': 475,\n",
       " 'tonight': 1266,\n",
       " 'believe': 2339,\n",
       " 'we': 19166,\n",
       " 'share': 198,\n",
       " 'an': 5985,\n",
       " 'art': 207,\n",
       " 'instructor': 5,\n",
       " 'know': 20121,\n",
       " 'chastity': 9,\n",
       " 'looks': 944,\n",
       " 'things': 2580,\n",
       " 'worked': 405,\n",
       " 'huh': 1562,\n",
       " 'hi': 898,\n",
       " 'who': 6493,\n",
       " 'knows': 926,\n",
       " \"i've\": 4923,\n",
       " 'ever': 2747,\n",
       " 'heard': 1323,\n",
       " 'her': 8546,\n",
       " 'is': 26869,\n",
       " 'that': 32433,\n",
       " \"she'd\": 179,\n",
       " 'dip': 20,\n",
       " 'before': 2703,\n",
       " 'dating': 54,\n",
       " 'a': 65240,\n",
       " 'guy': 2262,\n",
       " 'smokes': 25,\n",
       " 'kind': 1896,\n",
       " 'likes': 261,\n",
       " 'pretty': 1247,\n",
       " 'ones': 314,\n",
       " 'lesbian': 16,\n",
       " 'found': 1155,\n",
       " 'picture': 393,\n",
       " 'jared': 2,\n",
       " 'leto': 4,\n",
       " 'in': 31290,\n",
       " 'drawers': 12,\n",
       " 'sure': 4104,\n",
       " \"she's\": 2727,\n",
       " 'harboring': 5,\n",
       " 'samesex': 1,\n",
       " 'tendencies': 5,\n",
       " \"workin'\": 72,\n",
       " 'on': 18034,\n",
       " 'it': 43785,\n",
       " \"doesn't\": 2120,\n",
       " 'seem': 515,\n",
       " 'be': 18118,\n",
       " \"goin'\": 393,\n",
       " 'for': 21603,\n",
       " 'him': 11181,\n",
       " 'really': 4269,\n",
       " 'wanna': 888,\n",
       " \"can't\": 6184,\n",
       " 'unless': 371,\n",
       " 'sister': 443,\n",
       " 'goes': 645,\n",
       " \"eber's\": 1,\n",
       " 'deep': 239,\n",
       " 'conditioner': 7,\n",
       " 'every': 1722,\n",
       " 'two': 3375,\n",
       " 'days': 1017,\n",
       " 'and': 41365,\n",
       " 'use': 1066,\n",
       " 'blowdryer': 2,\n",
       " 'without': 1137,\n",
       " 'diffuser': 1,\n",
       " 'attachment': 5,\n",
       " 'hair': 414,\n",
       " 'look': 5472,\n",
       " 'sweet': 329,\n",
       " 'word': 785,\n",
       " 'as': 7155,\n",
       " 'gentleman': 124,\n",
       " 'counted': 23,\n",
       " 'help': 2459,\n",
       " 'cause': 463,\n",
       " 'thug': 4,\n",
       " 'are': 15445,\n",
       " 'obviously': 171,\n",
       " 'failing': 16,\n",
       " \"aren't\": 929,\n",
       " 'going': 8342,\n",
       " 'our': 3778,\n",
       " 'date': 339,\n",
       " 'got': 10602,\n",
       " 'something': 5072,\n",
       " 'mind': 1632,\n",
       " 'where': 5255,\n",
       " 'there': 9597,\n",
       " \"there's\": 3588,\n",
       " 'someone': 1541,\n",
       " 'think': 10060,\n",
       " 'might': 1767,\n",
       " 'little': 4158,\n",
       " 'find': 2707,\n",
       " 'wench': 7,\n",
       " 'plan': 459,\n",
       " 'progressing': 3,\n",
       " 'forget': 963,\n",
       " 'french': 219,\n",
       " 'because': 3076,\n",
       " \"it's\": 16723,\n",
       " 'such': 979,\n",
       " 'nice': 1692,\n",
       " \"don't\": 22742,\n",
       " 'want': 10357,\n",
       " 'though': 604,\n",
       " 'useful': 53,\n",
       " 'stores': 28,\n",
       " 'much': 3388,\n",
       " 'does': 2257,\n",
       " 'champagne': 78,\n",
       " 'cost': 226,\n",
       " 'chat': 45,\n",
       " 'life': 2634,\n",
       " 'point': 936,\n",
       " 'head': 1107,\n",
       " 'right': 9346,\n",
       " 'see': 7640,\n",
       " 'ready': 832,\n",
       " 'quiz': 9,\n",
       " \"c'esc\": 1,\n",
       " 'ma': 223,\n",
       " 'tete': 1,\n",
       " 'let': 3462,\n",
       " 'can': 10542,\n",
       " 'gosh': 67,\n",
       " 'only': 3750,\n",
       " 'could': 5433,\n",
       " 'kat': 35,\n",
       " 'boyfriend': 178,\n",
       " 'shame': 108,\n",
       " 'unsolved': 10,\n",
       " 'mystery': 63,\n",
       " 'used': 1230,\n",
       " 'popular': 77,\n",
       " 'when': 6573,\n",
       " 'started': 550,\n",
       " 'high': 558,\n",
       " 'school': 983,\n",
       " 'just': 14647,\n",
       " 'sick': 606,\n",
       " 'or': 5940,\n",
       " 'why': 7950,\n",
       " 'seems': 529,\n",
       " 'easy': 780,\n",
       " 'enough': 1715,\n",
       " 'thing': 3764,\n",
       " 'cameron': 23,\n",
       " 'at': 9903,\n",
       " 'mercy': 51,\n",
       " 'particularly': 66,\n",
       " 'hideous': 15,\n",
       " 'breed': 19,\n",
       " 'loser': 84,\n",
       " 'until': 964,\n",
       " 'fault': 336,\n",
       " \"didn't\": 5444,\n",
       " 'proper': 94,\n",
       " 'introduction': 11,\n",
       " 'asking': 441,\n",
       " 'cute': 184,\n",
       " \"what's\": 4522,\n",
       " 'name': 1952,\n",
       " 'again': 2190,\n",
       " \"'bout\": 252,\n",
       " 'try': 1470,\n",
       " 'some': 6054,\n",
       " 'cuisine': 7,\n",
       " 'saturday': 134,\n",
       " 'night': 2541,\n",
       " 'hacking': 10,\n",
       " 'gagging': 5,\n",
       " 'spitting': 12,\n",
       " 'part': 907,\n",
       " 'please': 2345,\n",
       " 'thought': 2909,\n",
       " \"we'd\": 427,\n",
       " 'start': 1081,\n",
       " 'pronunciation': 1,\n",
       " 'make': 4395,\n",
       " 'quick': 247,\n",
       " 'roxanne': 1,\n",
       " 'korrine': 1,\n",
       " 'andrew': 37,\n",
       " 'barrett': 12,\n",
       " 'having': 792,\n",
       " 'incredibly': 41,\n",
       " 'horrendous': 3,\n",
       " 'public': 223,\n",
       " 'break': 607,\n",
       " 'up': 10878,\n",
       " 'quad': 2,\n",
       " \"'\": 43,\n",
       " 're': 41,\n",
       " 'sophomore': 8,\n",
       " 'prom': 76,\n",
       " 'home': 2035,\n",
       " \"'til\": 79,\n",
       " 'twenty': 571,\n",
       " 'minutes': 879,\n",
       " \"i'd\": 2748,\n",
       " 'give': 3274,\n",
       " 'private': 300,\n",
       " 'line': 535,\n",
       " 'joey': 112,\n",
       " 'sometimes': 671,\n",
       " 'wonder': 352,\n",
       " 'guys': 1373,\n",
       " \"we're\": 4291,\n",
       " 'supposed': 873,\n",
       " 'actually': 794,\n",
       " 'bianca': 19,\n",
       " 'highlights': 6,\n",
       " 'dorsey': 6,\n",
       " 'include': 34,\n",
       " 'dooropening': 1,\n",
       " 'coatholding': 1,\n",
       " 'combination': 40,\n",
       " \"he'd\": 384,\n",
       " 'different': 765,\n",
       " 'he': 17710,\n",
       " 'oily': 4,\n",
       " 'dry': 127,\n",
       " 'practically': 96,\n",
       " 'proposed': 31,\n",
       " 'same': 1382,\n",
       " 'dermatologist': 2,\n",
       " 'mean': 4643,\n",
       " 'dr': 777,\n",
       " 'bonchowski': 1,\n",
       " 'great': 2045,\n",
       " \"he's\": 6167,\n",
       " 'exactly': 924,\n",
       " 'relevant': 16,\n",
       " 'party': 593,\n",
       " 'conversation': 131,\n",
       " 'would': 5701,\n",
       " 'getting': 1590,\n",
       " 'drink': 782,\n",
       " 'here': 11824,\n",
       " 'change': 724,\n",
       " 'deal': 952,\n",
       " 't': 66,\n",
       " 'talk': 3002,\n",
       " 'concentrating': 13,\n",
       " 'awfully': 54,\n",
       " 'hard': 991,\n",
       " 'considering': 65,\n",
       " 'gym': 24,\n",
       " 'class': 302,\n",
       " 'hey': 2394,\n",
       " 'cheeks': 16,\n",
       " 'agent': 276,\n",
       " 'says': 1014,\n",
       " 'shot': 729,\n",
       " 'being': 1464,\n",
       " 'prada': 3,\n",
       " 'next': 1214,\n",
       " 'year': 785,\n",
       " 'neat': 44,\n",
       " 'gay': 81,\n",
       " 'cruise': 22,\n",
       " \"i'll\": 6374,\n",
       " 'uniform': 71,\n",
       " 'queen': 132,\n",
       " 'harry': 418,\n",
       " 'hopefully': 22,\n",
       " 'bogey': 8,\n",
       " \"lowenbrau's\": 1,\n",
       " 'expensive': 81,\n",
       " 'perm': 7,\n",
       " 'patrick': 77,\n",
       " 'woman': 1058,\n",
       " 'complete': 152,\n",
       " 'fruitloop': 1,\n",
       " 'completely': 281,\n",
       " 'damage': 119,\n",
       " 'send': 595,\n",
       " 'therapy': 67,\n",
       " 'forever': 205,\n",
       " 'set': 729,\n",
       " 'looked': 409,\n",
       " 'beautiful': 776,\n",
       " 'last': 2416,\n",
       " 'guess': 1820,\n",
       " 'will': 6168,\n",
       " 'experiences': 21,\n",
       " 'trust': 708,\n",
       " 'people': 3700,\n",
       " 'keep': 2149,\n",
       " 'locked': 176,\n",
       " 'away': 2267,\n",
       " 'dark': 357,\n",
       " 'experience': 204,\n",
       " 'anything': 3214,\n",
       " 'protecting': 57,\n",
       " 'stupid': 551,\n",
       " 'repeat': 93,\n",
       " 'mistakes': 65,\n",
       " 'own': 1585,\n",
       " 'decisions': 38,\n",
       " \"would've\": 150,\n",
       " 'instead': 247,\n",
       " 'helping': 123,\n",
       " 'daddy': 519,\n",
       " 'hold': 899,\n",
       " 'hostage': 44,\n",
       " 'tell': 6338,\n",
       " 'once': 1151,\n",
       " 'afterwards': 42,\n",
       " 'told': 2518,\n",
       " 'anymore': 708,\n",
       " \"wasn't\": 1575,\n",
       " 'pissed': 121,\n",
       " 'broke': 296,\n",
       " 'said': 3474,\n",
       " 'everyone': 602,\n",
       " 'doing': 2905,\n",
       " 'now': 9115,\n",
       " 'back': 5726,\n",
       " 'hate': 744,\n",
       " 'total': 121,\n",
       " 'babe': 114,\n",
       " '9th': 8,\n",
       " 'month': 302,\n",
       " 'went': 1359,\n",
       " 'wish': 733,\n",
       " 'luxury': 17,\n",
       " 'asked': 701,\n",
       " 'won': 195,\n",
       " 'care': 1530,\n",
       " 'firm': 85,\n",
       " 'believer': 13,\n",
       " 'reasons': 107,\n",
       " 'else': 1685,\n",
       " 's': 54,\n",
       " 'sit': 795,\n",
       " 'susie': 65,\n",
       " 'welcome': 285,\n",
       " 'act': 392,\n",
       " 'too': 4586,\n",
       " 'any': 3944,\n",
       " 'totally': 256,\n",
       " 'apeshit': 4,\n",
       " 'social': 96,\n",
       " 'advice': 170,\n",
       " 'from': 6355,\n",
       " 'unbalanced': 3,\n",
       " 'yeah': 6504,\n",
       " 'freak': 103,\n",
       " 'friend': 1299,\n",
       " \"mandella's\": 1,\n",
       " 'since': 1048,\n",
       " 'allowed': 119,\n",
       " 'should': 3217,\n",
       " 'obsess': 1,\n",
       " 'over': 3817,\n",
       " 'dead': 1868,\n",
       " 'shakespeare': 34,\n",
       " 'maybe': 3712,\n",
       " \"you've\": 2611,\n",
       " 'even': 2944,\n",
       " 'means': 594,\n",
       " 'least': 789,\n",
       " 'clouted': 1,\n",
       " 'fen': 1,\n",
       " 'sucked': 36,\n",
       " 'hedgepig': 1,\n",
       " 'wretched': 20,\n",
       " \"lowenstein's\": 2,\n",
       " 'normal': 201,\n",
       " 'busy': 300,\n",
       " 'listening': 227,\n",
       " 'bitches': 33,\n",
       " 'prozac': 5,\n",
       " 'ruining': 22,\n",
       " \"life'\": 5,\n",
       " \"won't\": 2441,\n",
       " 'torture': 56,\n",
       " 'suck': 88,\n",
       " 'oh': 7260,\n",
       " 'bothering': 66,\n",
       " 'ask': 1749,\n",
       " 'gigglepuss': 4,\n",
       " 'playing': 395,\n",
       " 'club': 207,\n",
       " 'skunk': 16,\n",
       " 'becoming': 68,\n",
       " 'bra': 18,\n",
       " 'potential': 44,\n",
       " 'smack': 22,\n",
       " 'way': 4623,\n",
       " 'nowhere': 136,\n",
       " \"where've\": 21,\n",
       " 'captain': 689,\n",
       " 'oppression': 6,\n",
       " 'men': 1091,\n",
       " 'missing': 230,\n",
       " 'fine': 1681,\n",
       " 'prisoner': 49,\n",
       " 'house': 1603,\n",
       " 'daughter': 419,\n",
       " 'possession': 62,\n",
       " 'end': 786,\n",
       " 'hot': 467,\n",
       " 'rod': 50,\n",
       " 'bend': 45,\n",
       " 'rules': 177,\n",
       " 'has': 3361,\n",
       " 'discuss': 145,\n",
       " 'tomorrow': 1139,\n",
       " \"she'll\": 327,\n",
       " 'scare': 125,\n",
       " 'them': 5632,\n",
       " 'promise': 469,\n",
       " 'boys': 534,\n",
       " 'present': 238,\n",
       " 'minute': 1021,\n",
       " 'wear': 318,\n",
       " 'belly': 39,\n",
       " 'starting': 206,\n",
       " \"kat's\": 1,\n",
       " 'expect': 415,\n",
       " 'otherwise': 151,\n",
       " 'known': 403,\n",
       " 'orgy': 9,\n",
       " 'must': 2421,\n",
       " 'were': 5809,\n",
       " 'attempting': 15,\n",
       " 'small': 414,\n",
       " 'study': 153,\n",
       " 'group': 170,\n",
       " 'friends': 950,\n",
       " \"where're\": 36,\n",
       " 'fair': 293,\n",
       " 'mutant': 19,\n",
       " 'neither': 245,\n",
       " 'sleep': 802,\n",
       " 'starts': 144,\n",
       " 'discussion': 47,\n",
       " 'upset': 254,\n",
       " 'boy': 1384,\n",
       " 'sent': 483,\n",
       " \"'em\": 1220,\n",
       " 'through': 1685,\n",
       " 'padua': 1,\n",
       " 'girls': 443,\n",
       " 'tall': 81,\n",
       " 'decent': 90,\n",
       " 'body': 571,\n",
       " 'other': 2533,\n",
       " 'kinda': 297,\n",
       " 'short': 245,\n",
       " 'undersexed': 1,\n",
       " 'fan': 93,\n",
       " 'couple': 715,\n",
       " 'minors': 10,\n",
       " 'come': 6355,\n",
       " 'pegged': 11,\n",
       " 'preteen': 1,\n",
       " 'bellybutton': 2,\n",
       " 'ring': 250,\n",
       " 'pleasure': 243,\n",
       " 'brucie': 1,\n",
       " 'best': 1282,\n",
       " 'case': 939,\n",
       " 'scenario': 17,\n",
       " 'payroll': 22,\n",
       " 'awhile': 121,\n",
       " 'humiliated': 21,\n",
       " 'sacrifice': 42,\n",
       " 'yourself': 1666,\n",
       " 'altar': 10,\n",
       " 'dignity': 28,\n",
       " 'score': 86,\n",
       " 'm': 62,\n",
       " 'buttholus': 1,\n",
       " 'extremus': 1,\n",
       " 'making': 707,\n",
       " 'progress': 68,\n",
       " 'hell': 2033,\n",
       " \"'guy\": 1,\n",
       " 'picks': 29,\n",
       " 'girl': 1688,\n",
       " 'carries': 15,\n",
       " 'while': 1109,\n",
       " 'talking': 1722,\n",
       " 'extremely': 76,\n",
       " 'unfortunate': 47,\n",
       " 'maneuver': 16,\n",
       " 'whole': 1321,\n",
       " 'already': 1076,\n",
       " 'favorite': 157,\n",
       " 'band': 125,\n",
       " 'assail': 1,\n",
       " 'ears': 106,\n",
       " 'fortyone': 10,\n",
       " 'uncle': 274,\n",
       " 'lung': 22,\n",
       " 'cancer': 97,\n",
       " 'issue': 105,\n",
       " 'number': 615,\n",
       " 'hates': 98,\n",
       " 'smokers': 4,\n",
       " 'kidding': 380,\n",
       " \"he'll\": 730,\n",
       " 'piss': 102,\n",
       " 'himself': 526,\n",
       " 'joy': 55,\n",
       " 'ultimate': 32,\n",
       " 'kiss': 290,\n",
       " 'ass': 811,\n",
       " 'bent': 32,\n",
       " \"we'll\": 2050,\n",
       " 'schoolwide': 1,\n",
       " 'blow': 320,\n",
       " 'golden': 50,\n",
       " 'opportunity': 111,\n",
       " 'katarina': 3,\n",
       " 'choice': 322,\n",
       " 'besides': 373,\n",
       " 'enemy': 144,\n",
       " 'orchestrating': 1,\n",
       " 'battle': 98,\n",
       " 'position': 191,\n",
       " 'power': 551,\n",
       " 'pretend': 118,\n",
       " 'calling': 367,\n",
       " 'shots': 75,\n",
       " 'setting': 57,\n",
       " 'time': 6097,\n",
       " 'woo': 4,\n",
       " 'involved': 269,\n",
       " 'gotta': 1683,\n",
       " 'few': 1101,\n",
       " 'clients': 44,\n",
       " 'wall': 208,\n",
       " 'street': 514,\n",
       " 'hated': 105,\n",
       " 'those': 2390,\n",
       " 'outrank': 2,\n",
       " 'strictly': 43,\n",
       " 'alist': 3,\n",
       " 'by': 3702,\n",
       " 'side': 666,\n",
       " 'his': 6669,\n",
       " 'reputation': 88,\n",
       " \"we've\": 1314,\n",
       " 'serious': 542,\n",
       " 'man': 5110,\n",
       " 'whacked': 21,\n",
       " 'sold': 145,\n",
       " 'liver': 23,\n",
       " 'black': 536,\n",
       " 'market': 109,\n",
       " 'buy': 616,\n",
       " 'new': 1998,\n",
       " 'speakers': 5,\n",
       " 'felons': 3,\n",
       " 'honors': 14,\n",
       " 'biology': 8,\n",
       " 'criminal': 130,\n",
       " 'lit': 30,\n",
       " 'state': 381,\n",
       " 'trooper': 12,\n",
       " 'fire': 575,\n",
       " 'alcatraz': 1,\n",
       " 'thrives': 1,\n",
       " 'danger': 159,\n",
       " 'makes': 871,\n",
       " 'unlikely': 19,\n",
       " 'still': 2627,\n",
       " 'teach': 212,\n",
       " 'dazzle': 2,\n",
       " 'charm': 42,\n",
       " 'falls': 84,\n",
       " 'love': 2905,\n",
       " 'mewling': 1,\n",
       " 'rampalian': 1,\n",
       " 'wretch': 4,\n",
       " 'herself': 178,\n",
       " \"bianca's\": 2,\n",
       " 'minor': 49,\n",
       " 'encounter': 19,\n",
       " 'shrew': 1,\n",
       " 'consecrate': 1,\n",
       " 'chance': 761,\n",
       " 'signed': 85,\n",
       " 'tutor': 7,\n",
       " \"mom's\": 61,\n",
       " 'canada': 36,\n",
       " 'permashitgrin': 1,\n",
       " 'moron': 48,\n",
       " 'twelve': 278,\n",
       " 'model': 66,\n",
       " 'mostly': 136,\n",
       " 'regional': 12,\n",
       " 'rumored': 2,\n",
       " 'big': 2008,\n",
       " 'tube': 31,\n",
       " 'sock': 20,\n",
       " 'ad': 41,\n",
       " 'coming': 1357,\n",
       " 'shiteating': 2,\n",
       " 'grin': 4,\n",
       " \"they're\": 2786,\n",
       " 'bred': 12,\n",
       " 'their': 2173,\n",
       " 'mothers': 34,\n",
       " 'liked': 292,\n",
       " 'grandmothers': 1,\n",
       " 'gene': 30,\n",
       " 'pool': 131,\n",
       " 'rarely': 23,\n",
       " 'diluted': 1,\n",
       " 'haircut': 28,\n",
       " 'matter': 1245,\n",
       " 'older': 127,\n",
       " 'impossibility': 5,\n",
       " 'stratford': 8,\n",
       " 'burn': 145,\n",
       " 'pine': 14,\n",
       " 'perish': 9,\n",
       " 'these': 2570,\n",
       " 'seen': 1325,\n",
       " 'horse': 187,\n",
       " 'jack': 745,\n",
       " 'off': 3674,\n",
       " 'clint': 6,\n",
       " 'eastwood': 4,\n",
       " 'thousand': 780,\n",
       " 'most': 1104,\n",
       " 'evil': 240,\n",
       " 'many': 1202,\n",
       " 'thirtytwo': 13,\n",
       " 'old': 2118,\n",
       " 'outnumbered': 5,\n",
       " 'cows': 26,\n",
       " 'live': 1262,\n",
       " 'north': 173,\n",
       " \"how'd\": 327,\n",
       " 'which': 1306,\n",
       " 'dakota': 14,\n",
       " \"c'mon\": 385,\n",
       " 'tour': 71,\n",
       " 'worst': 172,\n",
       " 'kissed': 49,\n",
       " \"makin'\": 68,\n",
       " 'headway': 1,\n",
       " 'needs': 456,\n",
       " 'cool': 460,\n",
       " 'day': 2270,\n",
       " 'suns': 7,\n",
       " 'direct': 87,\n",
       " 'quote': 29,\n",
       " 'don': 128,\n",
       " 'decided': 244,\n",
       " 'nail': 69,\n",
       " 'drunk': 269,\n",
       " 'remember': 1867,\n",
       " \"what'd\": 310,\n",
       " 'partial': 19,\n",
       " \"'re\": 9,\n",
       " 'noodles': 6,\n",
       " 'book': 553,\n",
       " 'around': 2572,\n",
       " 'chicks': 42,\n",
       " 'play': 1026,\n",
       " 'instruments': 22,\n",
       " 'retrieved': 3,\n",
       " 'certain': 294,\n",
       " 'pieces': 124,\n",
       " 'information': 326,\n",
       " 'miss': 1191,\n",
       " \"you'll\": 2075,\n",
       " 'helpful': 42,\n",
       " \"what've\": 17,\n",
       " 'non': 30,\n",
       " 'prisonmovie': 1,\n",
       " 'type': 192,\n",
       " 'leave': 1798,\n",
       " 'alone': 1021,\n",
       " 'ya': 1241,\n",
       " 'goin': 16,\n",
       " 'running': 537,\n",
       " 'rest': 759,\n",
       " 'noticed': 146,\n",
       " 'featured': 2,\n",
       " 'kmart': 4,\n",
       " 'spread': 62,\n",
       " 'elbow': 11,\n",
       " 'tough': 282,\n",
       " 'vintage': 6,\n",
       " \"haven't\": 1316,\n",
       " 'reading': 202,\n",
       " 'sassy': 9,\n",
       " 'barbie': 9,\n",
       " \"n'\": 10,\n",
       " 'ken': 28,\n",
       " 'shit': 2141,\n",
       " 'limothe': 1,\n",
       " 'flowers': 117,\n",
       " 'another': 1595,\n",
       " 'hundred': 1063,\n",
       " 'tux': 6,\n",
       " 'human': 416,\n",
       " \"deal's\": 16,\n",
       " 'bucks': 289,\n",
       " 'upped': 6,\n",
       " 'price': 225,\n",
       " 'under': 819,\n",
       " 'control': 408,\n",
       " 'acts': 33,\n",
       " 'crazed': 7,\n",
       " 'image': 72,\n",
       " 'watching': 294,\n",
       " 'bitch': 384,\n",
       " 'trash': 45,\n",
       " 'car': 1440,\n",
       " 'count': 246,\n",
       " 'shell': 36,\n",
       " 'fifty': 413,\n",
       " 'results': 42,\n",
       " 'take': 5295,\n",
       " \"isn't\": 2196,\n",
       " 'negotiation': 12,\n",
       " 'thirty': 324,\n",
       " 'gets': 763,\n",
       " 'catch': 390,\n",
       " 'pay': 816,\n",
       " 'verona': 13,\n",
       " 'pick': 564,\n",
       " 'tab': 18,\n",
       " 'cake': 79,\n",
       " 'money': 2551,\n",
       " 'sparky': 6,\n",
       " 'whatever': 778,\n",
       " 'legs': 154,\n",
       " 'rack': 20,\n",
       " 'higher': 71,\n",
       " 'better': 2562,\n",
       " 'fuck': 1864,\n",
       " 'heavily': 17,\n",
       " 'invested': 13,\n",
       " 'took': 1019,\n",
       " 'bathes': 2,\n",
       " 'together': 1049,\n",
       " 'kids': 805,\n",
       " 'uh': 1507,\n",
       " \"helpin'\": 6,\n",
       " 'recruit': 12,\n",
       " \"who'll\": 32,\n",
       " \"who's\": 1014,\n",
       " 'job': 1474,\n",
       " 'purpose': 122,\n",
       " 'insane': 162,\n",
       " 'run': 1110,\n",
       " 'idea': 1215,\n",
       " 'interested': 364,\n",
       " 'nope': 142,\n",
       " 'came': 1493,\n",
       " 'lost': 836,\n",
       " 'honey': 650,\n",
       " \"haven't'\": 1,\n",
       " 'progressed': 3,\n",
       " 'fullon': 3,\n",
       " 'hallucinations': 11,\n",
       " 'william': 102,\n",
       " 'meet': 1019,\n",
       " 'us': 5530,\n",
       " 'looking': 1323,\n",
       " 'wrong': 1770,\n",
       " 'perspective': 21,\n",
       " 'statement': 82,\n",
       " 'dress': 210,\n",
       " 'anyway': 974,\n",
       " 'sound': 452,\n",
       " 'betty': 148,\n",
       " 'archie': 17,\n",
       " 'taking': 728,\n",
       " 'veronica': 49,\n",
       " 'dates': 46,\n",
       " 'imagine': 300,\n",
       " 'bastion': 3,\n",
       " 'commercial': 42,\n",
       " 'excess': 9,\n",
       " 'puked': 2,\n",
       " 'rejected': 16,\n",
       " 'favor': 295,\n",
       " 'backfired': 1,\n",
       " \"where's\": 842,\n",
       " 'done': 1628,\n",
       " 'officially': 29,\n",
       " 'opposed': 30,\n",
       " 'suburban': 8,\n",
       " 'activity': 37,\n",
       " 'cares': 129,\n",
       " \"this'll\": 43,\n",
       " 'work': 2599,\n",
       " 'appreciate': 246,\n",
       " 'efforts': 21,\n",
       " 'toward': 100,\n",
       " 'speedy': 3,\n",
       " 'death': 706,\n",
       " 'consuming': 5,\n",
       " 'heterosexuality': 2,\n",
       " 'proven': 22,\n",
       " 'gone': 986,\n",
       " 'huge': 88,\n",
       " 'raging': 5,\n",
       " 'fit': 172,\n",
       " 'sarah': 88,\n",
       " 'lawrence': 15,\n",
       " 'insists': 11,\n",
       " 'maledominated': 1,\n",
       " 'puking': 8,\n",
       " 'frat': 6,\n",
       " 'golf': 58,\n",
       " 'team': 237,\n",
       " 'foul': 30,\n",
       " 'during': 185,\n",
       " 'sex': 420,\n",
       " 'realize': 267,\n",
       " 'institution': 39,\n",
       " 'severely': 7,\n",
       " 'lacking': 13,\n",
       " 'killing': 282,\n",
       " 'beyond': 153,\n",
       " 'scope': 19,\n",
       " 'teenage': 16,\n",
       " 'obsessions': 4,\n",
       " 'venturing': 2,\n",
       " 'far': 809,\n",
       " 'past': 364,\n",
       " 'daytime': 13,\n",
       " 'show': 1325,\n",
       " 'fodder': 2,\n",
       " 'entering': 43,\n",
       " 'world': 1460,\n",
       " 'very': 3606,\n",
       " 'attempted': 18,\n",
       " 'slit': 18,\n",
       " 'mandella': 1,\n",
       " 'eat': 724,\n",
       " 'starving': 41,\n",
       " 'slow': 241,\n",
       " 'die': 914,\n",
       " 'block': 97,\n",
       " 'e': 42,\n",
       " 'incapable': 15,\n",
       " 'interesting': 295,\n",
       " 'pat': 48,\n",
       " 'porn': 18,\n",
       " 'movies': 217,\n",
       " 'random': 41,\n",
       " 'skid': 4,\n",
       " 'dare': 91,\n",
       " 'freshman': 14,\n",
       " 'yearbook': 5,\n",
       " 'veggie': 1,\n",
       " 'burger': 31,\n",
       " 'burnt': 25,\n",
       " 'object': 71,\n",
       " 'grill': 9,\n",
       " 'fucked': 311,\n",
       " 'fell': 183,\n",
       " 'extra': 149,\n",
       " 'cash': 275,\n",
       " 'asshole': 260,\n",
       " 'paid': 319,\n",
       " 'fender': 9,\n",
       " 'strat': 1,\n",
       " 'bought': 255,\n",
       " 'down': 4128,\n",
       " 'payment': 40,\n",
       " 'bonus': 31,\n",
       " 'sleeping': 173,\n",
       " 'person': 636,\n",
       " 'truly': 130,\n",
       " 'knew': 1386,\n",
       " 'setup': 41,\n",
       " 'wait': 1822,\n",
       " 'worse': 374,\n",
       " 'adorable': 29,\n",
       " 'lived': 252,\n",
       " 'grandfather': 71,\n",
       " 'died': 565,\n",
       " 'stayed': 122,\n",
       " 'jail': 265,\n",
       " 'marilyn': 8,\n",
       " 'manson': 10,\n",
       " 'slept': 98,\n",
       " 'spice': 32,\n",
       " 'spent': 196,\n",
       " 'sitting': 279,\n",
       " 'grandma': 55,\n",
       " 'couch': 42,\n",
       " 'wheel': 84,\n",
       " 'fortune': 115,\n",
       " \"grandmother's\": 12,\n",
       " 'sorry': 3094,\n",
       " 'questioned': 27,\n",
       " 'motives': 15,\n",
       " \"scurvy's\": 1,\n",
       " 'convicted': 18,\n",
       " \"where'd\": 194,\n",
       " 'nothing': 2889,\n",
       " 'company': 396,\n",
       " 'answer': 588,\n",
       " 'question': 709,\n",
       " 'anyone': 906,\n",
       " 'motive': 35,\n",
       " 'create': 85,\n",
       " 'drama': 18,\n",
       " 'rumor': 33,\n",
       " 'tradition': 40,\n",
       " 'request': 78,\n",
       " 'command': 166,\n",
       " 'amazingly': 5,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words: list[str]):\n",
    "    return list(\n",
    "        filter(\n",
    "            lambda x: x != '' and len(set(x)) > 1 and x.isalpha() and len(x) < 15 and word_frequency[x] > 5,\n",
    "            words\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_preprocessed = preprocess(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2504600, 3060706)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_list_preprocessed), len(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'do', 'not', 'they', 'do']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list_preprocessed[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Test Data**\n",
    "\n",
    "Let's utilize the same 5k most frequent words from [this file.](../data/wordFrequency.xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'to', 'and', 'of', 'a', 'in', 'i', 'that', 'you', 'it'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.read_excel('../data/wordFrequency.xlsx', sheet_name='4 forms (219k)')\n",
    "\n",
    "words = df_words['word'].values\n",
    "\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to filter this words out of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list_training = list(filter(lambda x: x not in words, words_list_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172994"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_list_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/movie_lines_processed.txt', 'w') as f:\n",
    "    for word in words_list_preprocessed:\n",
    "        f.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/movie_lines_filtered.txt', 'w') as f:\n",
    "    for word in words_list_training:\n",
    "        f.write(f'{word}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating the N-grams**\n",
    "\n",
    "We are going to use the same functions already created in [this script.](parallel_generate_probs.py)\n",
    "\n",
    "Now, we are creating n-grams with the filtered set and \"with overfitting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 156537\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Processing chunk with 156537 words\n",
      "Processing chunk with 8 words\n",
      "Chunk processed with 8 words\n",
      "Processing chunk with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Chunk processed with 156537 words\n",
      "Elapsed time: 2.69s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/movie_lines_processed.txt 5 -o movie-lines/movie-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 10812\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Processing chunk with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Processing chunk with 2 words\n",
      "Chunk processed with 2 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Chunk processed with 10812 words\n",
      "Elapsed time: 0.96s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/movie_lines_filtered.txt 5 -o movie-lines/movie-lines-filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the probability file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/out/movie-lines/movie-lines_probs.json', 'r') as f:\n",
    "    import json\n",
    "    probs = json.load(f)\n",
    "\n",
    "with open('../data/out/movie-lines/movie-lines-filtered_probs.json', 'r') as f:\n",
    "    import json\n",
    "    probs_filtered = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading the Competitor Model**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48452, 48387)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/out/kaggle/unigram_probs.json', 'r') as f:\n",
    "    import json\n",
    "    probs_kaggle = json.load(f)\n",
    "\n",
    "with open('../data/out/kaggle/unigram-filtered_probs.json', 'r') as f:\n",
    "    import json\n",
    "    probs_kaggle_filtered = json.load(f)\n",
    "\n",
    "len(probs_kaggle), len(probs_kaggle_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(n, probs, test):\n",
    "    acc_exact = 0\n",
    "    acc_top3 = 0\n",
    "    acc_top5 = 0\n",
    "    total = 0\n",
    "\n",
    "    for word in test:\n",
    "        word = r'%'*(n-1) + str(word)\n",
    "        for idx in range(4, len(word)):\n",
    "            prev = word[idx - n:idx]\n",
    "            \n",
    "            nxt = word[idx]\n",
    "\n",
    "            if prev in probs:\n",
    "                if nxt == max(probs[prev], key=probs[prev].get):\n",
    "                    acc_exact += 1\n",
    "                if nxt in list(sorted(probs[prev], key=probs[prev].get, reverse=True))[:3]:\n",
    "                    acc_top3 += 1\n",
    "                if nxt in list(sorted(probs[prev], key=probs[prev].get, reverse=True))[:5]:\n",
    "                    acc_top5 += 1\n",
    "                total += 1\n",
    "\n",
    "    return acc_exact / total, acc_top3 / total, acc_top5 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5459449541284404, 0.8044036697247706, 0.9048807339449542)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3831352574985852, 0.6254843062992469, 0.7281354751643376)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs_filtered, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5012652736606175, 0.7915913527582966, 0.902176270696262)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.46188422082865116, 0.7519483814840323, 0.8671838184652191)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle_filtered, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Testing\n",
    "\n",
    "Let's put the \"overfitting\" models to the test with very unusual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaxial',\n",
       " 'dorsal',\n",
       " 'adaxial',\n",
       " 'ventral',\n",
       " 'acroscopic',\n",
       " 'basiscopic',\n",
       " 'abducent',\n",
       " 'abducting',\n",
       " 'adducent',\n",
       " 'adductive']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/wordnet_words.txt', 'r') as f:\n",
    "    wordnet_words = f.readlines()\n",
    "\n",
    "wordnet_words = list(map(lambda x: x.replace('\\n', ''), wordnet_words))\n",
    "\n",
    "wordnet_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.32417132195394727, 0.5454243462758624, 0.6508617587297018)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs, wordnet_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.342144021651953, 0.5484675351946313, 0.6486282332834615)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs_filtered, wordnet_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.39374438409685364, 0.666887361807857, 0.7838091011280917)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle, wordnet_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3944891874043767, 0.6674847139169303, 0.7841283066785766)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle_filtered, wordnet_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft testing\n",
    "\n",
    "Let's test the model with some common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'was', 'he', 'his']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words100k = []\n",
    "with open('../data/wiki-100k.txt', 'r', encoding='utf-8') as f:\n",
    "    curr = f.read().splitlines()\n",
    "\n",
    "    for line in curr: # reading all lines and ignoring comments\n",
    "        if line and line[0] != '#':\n",
    "            words100k.extend(line.split())\n",
    "    \n",
    "    words100k = np.array(words100k)\n",
    "\n",
    "words100k = [word for word in words100k if word.islower() and word.isalpha()]\n",
    "\n",
    "words100k = words100k[:10000]\n",
    "\n",
    "words100k[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4912456228114057, 0.7535498518490015, 0.8591603494054719)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs, words100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4369915289776063, 0.6777446951270654, 0.7767969470770779)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, probs_filtered, words100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4734987440591286, 0.7749787664672823, 0.8906698953683791)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle, words100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4565272450973298, 0.7584846949851654, 0.8762573268688039)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, probs_kaggle_filtered, words100k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "After extensive testing with many different datasets, the models that yielded the best results were composed of sets that took into consideration the frequency of the words in the english language, which made n-grams of frequent words to appear more.\n",
    "\n",
    "However, we start to see we are hitting a plateau. Given the semi-determinant aspect of languages, there are only so many prefixes that can be formed, and even less that are actively used on a daily basis. This is why we should pay attention to the accuracy of the \"overfitted\" model. Most of the time, the user will be inputting common words, thus we cannot take that information out of the model to test it. However, it's also important to test how it reacts to uncommon words and prefixes, hence the \"filtered\" probabilities also being used.\n",
    "\n",
    "Furthermore, there will be times when the user will attempt to type an uncommon word. We, as humans, are creating new words and bringing back old words all the time. There will be a level of error to these models that can't be overcome without advanced context, such as knowing all the previous words that were typed. Even  if we use majority voting models, they are all based on the same words with almost the same amount of context, as we saw the improvements were very subtle. Therefore, our focus for testing will be balancing out the model with the interface, such that, even in situations where the predictions are all wrong, the user will not be heavily penalized on their typing speed, but when the predictions are correct, their gains will be siginificant.\n",
    "\n",
    "Finally, given the compared datasets, we can see the Kaggle 4-grams usually had a slightly lower performance than the Movie Lines 5-grams, but showed a way better capacity at adapting to uncommon words and prefixes. \n",
    "\n",
    "Now, it's time to bring them to the real world. Let's put both to user testing on gaze typing and see which one works the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
