{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Majority Voting Algorithm**\n",
    "\n",
    "To test how far we can take the *n-gram* model for letter prediction, we are developing a majority voting algorithm. It will take into consideration `n` probability tables to predict a letter: starting from 1-grams, to 2, 3, all the way to n-grams. All the probabilities will be added and the most probable letter will be the prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/unigram_freq_processed.txt') as f:\n",
    "    unigram = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "      <th>#texts</th>\n",
       "      <th>%caps</th>\n",
       "      <th>blog</th>\n",
       "      <th>web</th>\n",
       "      <th>TVM</th>\n",
       "      <th>spok</th>\n",
       "      <th>fic</th>\n",
       "      <th>...</th>\n",
       "      <th>news</th>\n",
       "      <th>acad</th>\n",
       "      <th>blogPM</th>\n",
       "      <th>webPM</th>\n",
       "      <th>TVMPM</th>\n",
       "      <th>spokPM</th>\n",
       "      <th>ficPM</th>\n",
       "      <th>magPM</th>\n",
       "      <th>newsPM</th>\n",
       "      <th>acadPM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>50074257</td>\n",
       "      <td>483041</td>\n",
       "      <td>0.11</td>\n",
       "      <td>6272412</td>\n",
       "      <td>7101104</td>\n",
       "      <td>3784652</td>\n",
       "      <td>5769026</td>\n",
       "      <td>6311500</td>\n",
       "      <td>...</td>\n",
       "      <td>6582642</td>\n",
       "      <td>7447070</td>\n",
       "      <td>50480.69</td>\n",
       "      <td>55212.83</td>\n",
       "      <td>29550.39</td>\n",
       "      <td>45736.71</td>\n",
       "      <td>53341.69</td>\n",
       "      <td>53975.61</td>\n",
       "      <td>54070.43</td>\n",
       "      <td>62167.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>to</td>\n",
       "      <td>25557793</td>\n",
       "      <td>478977</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3579158</td>\n",
       "      <td>3590504</td>\n",
       "      <td>2911924</td>\n",
       "      <td>3427348</td>\n",
       "      <td>2871517</td>\n",
       "      <td>...</td>\n",
       "      <td>3013501</td>\n",
       "      <td>2978222</td>\n",
       "      <td>28805.25</td>\n",
       "      <td>27917.05</td>\n",
       "      <td>22736.17</td>\n",
       "      <td>27171.94</td>\n",
       "      <td>24268.65</td>\n",
       "      <td>25264.13</td>\n",
       "      <td>24753.18</td>\n",
       "      <td>24861.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>24821791</td>\n",
       "      <td>478727</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3211226</td>\n",
       "      <td>3458960</td>\n",
       "      <td>1828166</td>\n",
       "      <td>3325442</td>\n",
       "      <td>3064047</td>\n",
       "      <td>...</td>\n",
       "      <td>2995111</td>\n",
       "      <td>3633119</td>\n",
       "      <td>25844.11</td>\n",
       "      <td>26894.26</td>\n",
       "      <td>14274.24</td>\n",
       "      <td>26364.03</td>\n",
       "      <td>25895.82</td>\n",
       "      <td>26215.95</td>\n",
       "      <td>24602.12</td>\n",
       "      <td>30328.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>of</td>\n",
       "      <td>23605964</td>\n",
       "      <td>478144</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2952017</td>\n",
       "      <td>3462140</td>\n",
       "      <td>1486604</td>\n",
       "      <td>2678416</td>\n",
       "      <td>2330823</td>\n",
       "      <td>...</td>\n",
       "      <td>2893200</td>\n",
       "      <td>4517563</td>\n",
       "      <td>23757.98</td>\n",
       "      <td>26918.99</td>\n",
       "      <td>11607.33</td>\n",
       "      <td>21234.42</td>\n",
       "      <td>19698.97</td>\n",
       "      <td>26054.07</td>\n",
       "      <td>23765.01</td>\n",
       "      <td>37712.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>21889251</td>\n",
       "      <td>477421</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2783458</td>\n",
       "      <td>2827106</td>\n",
       "      <td>2519099</td>\n",
       "      <td>2716641</td>\n",
       "      <td>2749208</td>\n",
       "      <td>...</td>\n",
       "      <td>2959649</td>\n",
       "      <td>2229222</td>\n",
       "      <td>22401.41</td>\n",
       "      <td>21981.44</td>\n",
       "      <td>19669.01</td>\n",
       "      <td>21537.47</td>\n",
       "      <td>23234.95</td>\n",
       "      <td>24619.48</td>\n",
       "      <td>24310.83</td>\n",
       "      <td>18609.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank word      freq  #texts  %caps     blog      web      TVM     spok  \\\n",
       "0     1  the  50074257  483041   0.11  6272412  7101104  3784652  5769026   \n",
       "1     2   to  25557793  478977   0.02  3579158  3590504  2911924  3427348   \n",
       "2     3  and  24821791  478727   0.09  3211226  3458960  1828166  3325442   \n",
       "3     4   of  23605964  478144   0.01  2952017  3462140  1486604  2678416   \n",
       "4     5    a  21889251  477421   0.05  2783458  2827106  2519099  2716641   \n",
       "\n",
       "       fic  ...     news     acad    blogPM     webPM     TVMPM    spokPM  \\\n",
       "0  6311500  ...  6582642  7447070  50480.69  55212.83  29550.39  45736.71   \n",
       "1  2871517  ...  3013501  2978222  28805.25  27917.05  22736.17  27171.94   \n",
       "2  3064047  ...  2995111  3633119  25844.11  26894.26  14274.24  26364.03   \n",
       "3  2330823  ...  2893200  4517563  23757.98  26918.99  11607.33  21234.42   \n",
       "4  2749208  ...  2959649  2229222  22401.41  21981.44  19669.01  21537.47   \n",
       "\n",
       "      ficPM     magPM    newsPM    acadPM  \n",
       "0  53341.69  53975.61  54070.43  62167.47  \n",
       "1  24268.65  25264.13  24753.18  24861.93  \n",
       "2  25895.82  26215.95  24602.12  30328.95  \n",
       "3  19698.97  26054.07  23765.01  37712.21  \n",
       "4  23234.95  24619.48  24310.83  18609.35  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.read_excel('../data/wordFrequency.xlsx', sheet_name='4 forms (219k)')\n",
    "\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_words['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1182554"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info',\n",
       "  'info'],\n",
       " 1182554)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram = [word for word in unigram if word not in words]\n",
    "\n",
    "unigram = list(map(lambda x: x.replace(r'\\n', ''), unigram))\n",
    "\n",
    "unigram[:10], len(unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create different N-grams to constitute the majority voting algorithm!\n",
    "\n",
    "* 1-grams  \n",
    "* 2-grams  \n",
    "* 3-grams  \n",
    "* 4-grams  \n",
    "* 5-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 73909\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 10 words\n",
      "Chunk processed with 10 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Elapsed time: 1.98s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/unigram_freq_processed.txt 4 -o majority/maj_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 73909\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 10 words\n",
      "Chunk processed with 10 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Elapsed time: 1.42s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/unigram_freq_processed.txt 3 -o majority/maj_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 73909\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 10 words\n",
      "Chunk processed with 10 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Elapsed time: 1.27s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/unigram_freq_processed.txt 2 -o majority/maj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 73909\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 10 words\n",
      "Chunk processed with 10 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Elapsed time: 1.07s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/unigram_freq_processed.txt 1 -o majority/maj_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 cores and chunk size of 73909\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Processing chunk with 10 words\n",
      "Chunk processed with 10 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Chunk processed with 73909 words\n",
      "Elapsed time: 2.37s\n"
     ]
    }
   ],
   "source": [
    "!python ./parallel_generate_probs.py ../data/unigram_freq_processed.txt 5 -o majority/maj_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to open all the generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/out/majority/maj_1_probs.json', 'r') as f:\n",
    "    maj_1_probs = json.load(f)\n",
    "\n",
    "with open('../data/out/majority/maj_2_probs.json', 'r') as f:\n",
    "    maj_2_probs = json.load(f)\n",
    "\n",
    "with open('../data/out/majority/maj_3_probs.json', 'r') as f:\n",
    "    maj_3_probs = json.load(f)\n",
    "\n",
    "with open('../data/out/majority/maj_4_probs.json', 'r') as f:\n",
    "    maj_4_probs = json.load(f)\n",
    "\n",
    "with open('../data/out/majority/maj_5_probs.json', 'r') as f:\n",
    "    maj_5_probs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Simple Evaluation**\n",
    "\n",
    "Before calculating the majority voting model, let's see how each one of the probability tables work by themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(n, probs, test):\n",
    "    acc_exact = 0\n",
    "    acc_top3 = 0\n",
    "    acc_top5 = 0\n",
    "    total = 0\n",
    "\n",
    "    for word in test:\n",
    "        word = r'%'*(n-1) + str(word)\n",
    "        for idx in range(4, len(word)):\n",
    "            prev = word[idx - n:idx]\n",
    "            \n",
    "            nxt = word[idx]\n",
    "\n",
    "            if prev in probs:\n",
    "                if nxt == max(probs[prev], key=probs[prev].get):\n",
    "                    acc_exact += 1\n",
    "                if nxt in list(sorted(probs[prev], key=probs[prev].get, reverse=True))[:3]:\n",
    "                    acc_top3 += 1\n",
    "                if nxt in list(sorted(probs[prev], key=probs[prev].get, reverse=True))[:5]:\n",
    "                    acc_top5 += 1\n",
    "                total += 1\n",
    "\n",
    "    return acc_exact / total, acc_top3 / total, acc_top5 / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.2432577382776586, 0.5249770150168557, 0.718817039534171)\n"
     ]
    }
   ],
   "source": [
    "# Test on 1-grams\n",
    "\n",
    "print(evaluate_model(1, maj_1_probs, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.33182818761641364, 0.6169216007224699, 0.7834283456567139)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(2, maj_2_probs, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.40943212929435663, 0.7184491742471076, 0.8531749536341959)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(3, maj_3_probs, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.46188422082865116, 0.7519483814840323, 0.8671838184652191)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(4, maj_4_probs, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4900968309859155, 0.7568588615023474, 0.8597417840375586)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(5, maj_5_probs, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolated, the `4-gram` achieved the best performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Majority Voting**\n",
    "\n",
    "With all the data in hands, the majority voting model works in a simple way:\n",
    "\n",
    "For each position in the string where we want to predict the next character, we separate all the prefixes, from 1 to 5 grams. Then, we check the probability tables and create a final dictionary with the total sum of the probabilities for each one of the letters. Then, we can check if the real letter was the one with the max probability, or if it was in the top 3 or top 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to evaluate the model using majority voting for each position\n",
    "\n",
    "def evaluate_model_majority(n_list, probs, test):\n",
    "    acc_exact = 0\n",
    "    acc_top3 = 0\n",
    "    acc_top5 = 0\n",
    "    total = 0\n",
    "\n",
    "    for word in test:\n",
    "        word = r'%'*(max(n_list)-1) + str(word)\n",
    "        for idx in range(max(n_list), len(word)):\n",
    "            total_probs = {letter : 0 for letter in 'abcdefghijklmnopqrstuvwxyz'}\n",
    "            for jdx in range(len(n_list)):\n",
    "                n = n_list[jdx]\n",
    "                prev = word[idx - n:idx]\n",
    "                if prev in probs[jdx]:\n",
    "                    for letter in probs[jdx][prev]:\n",
    "                        if letter in total_probs:\n",
    "                            total_probs[letter] += probs[jdx][prev][letter]\n",
    "        \n",
    "            nxt = word[idx]\n",
    "\n",
    "            if nxt == max(total_probs, key=total_probs.get):\n",
    "                acc_exact += 1\n",
    "            if nxt in list(sorted(total_probs, key=total_probs.get, reverse=True))[:3]:\n",
    "                acc_top3 += 1\n",
    "            if nxt in list(sorted(total_probs, key=total_probs.get, reverse=True))[:5]:\n",
    "                acc_top5 += 1\n",
    "            total += 1\n",
    "    \n",
    "    return acc_exact / total, acc_top3 / total, acc_top5 / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Let's test many combinations, removing the smallest n-gram in each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.47861705583023567, 0.7508029881987802, 0.8663971994658776)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    evaluate_model_majority(\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [maj_1_probs, maj_2_probs, maj_3_probs, maj_4_probs, maj_5_probs],\n",
    "        words\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48486051463423435, 0.7565051066440507, 0.8740481432025695)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    evaluate_model_majority(\n",
    "        [2, 3, 4, 5],\n",
    "        [maj_2_probs, maj_3_probs, maj_4_probs, maj_5_probs],\n",
    "        words\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48818073550110075, 0.7631816377350319, 0.8780179724999098)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    evaluate_model_majority(\n",
    "        [3, 4, 5],\n",
    "        [maj_3_probs, maj_4_probs, maj_5_probs],\n",
    "        words\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.48901079071781733, 0.7614132592298531, 0.869861777761738)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    evaluate_model_majority(\n",
    "        [4, 5],\n",
    "        [maj_4_probs, maj_5_probs],\n",
    "        words\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4833808509870439, 0.7461835504709661, 0.8525027969251867)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    evaluate_model_majority(\n",
    "        [5],\n",
    "        [maj_5_probs],\n",
    "        words\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best probabilities: 3, 4 and 5-grams combined!**\n",
    "\n",
    "However, the improvement was not that siginificant from the individual models. This might show us that we might be hitting a plateau with the simplest model.\n",
    "\n",
    "In a language, it is mandatory that the set of words is finite. Should we worry about overfitting to split the dataset? From the moment we have a big population of combination of prefixes, the samples might not be necessary. If we remove the most frequent words, we are not avoiding overfitting, but we are removing important data.\n",
    "\n",
    "Therefore, for a last test, we are going to approach creating a final dataset with a big conversational vocab, and smooth it with a sort of \"Laplace smoothing\", by also scanning the most frequent words of the language once. Then, we will test by training with a split, and then without to see the effects of \"overfitting\".\n",
    "\n",
    "Click [here](letter-n-gram-final-test.ipynb) to start reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top3_weights(txt, probs_list, n_max=5):\n",
    "    # get the n-gram prefix of the text\n",
    "    # if that n-gram isn't in the table, returns an empty dict\n",
    "    \n",
    "    total_probs = {letter: 0 for letter in 'abcdefghijklmnopqrstuvwxyz'}\n",
    "    for jdx in range(len(probs_list)):\n",
    "        prev = txt[-(n_max - len(probs_list) + jdx + 1):]\n",
    "        curr = probs_list[jdx]\n",
    "        print(prev)\n",
    "        if prev in curr:\n",
    "            for letter in curr[prev]:\n",
    "                if letter in total_probs:\n",
    "                    total_probs[letter] += curr[prev][letter]\n",
    "    # get the 3 most probable following letters\n",
    "    # if there are less than 3, gets all of them\n",
    "    top = list(sorted(total_probs, key=total_probs.get, reverse=True))[:3]\n",
    "    freq = list(sorted(total_probs.values(), reverse=True))[:3]\n",
    "\n",
    "    return list(zip(top, freq))\n",
    "\n",
    "def predicterV3(probs, n_max=5):\n",
    "    txt = r'%'*n_max\n",
    "    while (curr := input(\"Next Letter: \")) != '':\n",
    "        if curr == ' ':\n",
    "            txt = r'%'*n_max\n",
    "            continue\n",
    "\n",
    "        txt += curr\n",
    "        top3 = get_top3_weights(txt, probs)\n",
    "        print(f'\\n{txt} \\nBest 3 letters are: {top3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%a\n",
      "%%%a\n",
      "%%%%a\n",
      "\n",
      "%%%%%a \n",
      "Best 3 letters are: [('n', 0.3662795219454022), ('l', 0.3422860550125736), ('r', 0.29033261607860417)]\n",
      "%an\n",
      "%%an\n",
      "%%%an\n",
      "\n",
      "%%%%%an \n",
      "Best 3 letters are: [('t', 0.7494426159889584), ('a', 0.44399617793821), ('n', 0.3436670559507379)]\n",
      "ant\n",
      "%ant\n",
      "%%ant\n",
      "\n",
      "%%%%%ant \n",
      "Best 3 letters are: [('i', 1.3939060611747545), ('e', 0.36535602954739665), ('h', 0.2929822035375389)]\n",
      "nti\n",
      "anti\n",
      "%anti\n",
      "\n",
      "%%%%%anti \n",
      "Best 3 letters are: [('c', 0.4621550621617675), ('n', 0.414245894114128), ('a', 0.26224770295748684)]\n",
      "tic\n",
      "ntic\n",
      "antic\n",
      "\n",
      "%%%%%antic \n",
      "Best 3 letters are: [('a', 0.8327299744117148), ('i', 0.6509935855263003), ('o', 0.496559677392003)]\n",
      "ica\n",
      "tica\n",
      "ntica\n",
      "\n",
      "%%%%%antica \n",
      "Best 3 letters are: [('l', 1.7955009350036377), ('t', 0.8340451479694317), ('n', 0.15862271362173158)]\n"
     ]
    }
   ],
   "source": [
    "predicterV3([maj_3_probs, maj_4_probs, maj_5_probs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
